{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Preprocessing-and-Data-Augmentation\" data-toc-modified-id=\"Data-Preprocessing-and-Data-Augmentation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Preprocessing and Data Augmentation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Packages\" data-toc-modified-id=\"Import-Packages-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Import Packages</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Data-Visualization\" data-toc-modified-id=\"Data-Visualization-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Data Visualization</a></span></li></ul></li><li><span><a href=\"#Year-Counter\" data-toc-modified-id=\"Year-Counter-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Year Counter</a></span></li><li><span><a href=\"#KNN-Algorithms\" data-toc-modified-id=\"KNN-Algorithms-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>KNN Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#KNN-Preparation\" data-toc-modified-id=\"KNN-Preparation-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>KNN Preparation</a></span></li><li><span><a href=\"#Define-Functions\" data-toc-modified-id=\"Define-Functions-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Define Functions</a></span></li><li><span><a href=\"#KNN-Process\" data-toc-modified-id=\"KNN-Process-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>KNN Process</a></span></li><li><span><a href=\"#Apply-the-Algorithm-to-the-Whole-Data-Set\" data-toc-modified-id=\"Apply-the-Algorithm-to-the-Whole-Data-Set-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Apply the Algorithm to the Whole Data Set</a></span></li><li><span><a href=\"#Year-Counting\" data-toc-modified-id=\"Year-Counting-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Year Counting</a></span></li><li><span><a href=\"#Data-Visualization-After-KNN\" data-toc-modified-id=\"Data-Visualization-After-KNN-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Data Visualization After KNN</a></span></li></ul></li><li><span><a href=\"#K-Means-Algorithms\" data-toc-modified-id=\"K-Means-Algorithms-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>K-Means Algorithms</a></span></li><li><span><a href=\"#Synthetic-Minority-Oversampling-Technique\" data-toc-modified-id=\"Synthetic-Minority-Oversampling-Technique-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Synthetic Minority Oversampling Technique</a></span></li></ul></li><li><span><a href=\"#Random-Processes-and-Network\" data-toc-modified-id=\"Random-Processes-and-Network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Random Processes and Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Season-and-Year\" data-toc-modified-id=\"Season-and-Year-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Season and Year</a></span><ul class=\"toc-item\"><li><span><a href=\"#Season-Data-Creation\" data-toc-modified-id=\"Season-Data-Creation-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Season Data Creation</a></span></li><li><span><a href=\"#Network-Preparation\" data-toc-modified-id=\"Network-Preparation-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Network Preparation</a></span></li><li><span><a href=\"#Season-Data-Visualization\" data-toc-modified-id=\"Season-Data-Visualization-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Season Data Visualization</a></span></li></ul></li><li><span><a href=\"#Birth-Death-Process\" data-toc-modified-id=\"Birth-Death-Process-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Birth-Death Process</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#Gradient_Descent-Algorithm\" data-toc-modified-id=\"Gradient_Descent-Algorithm-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Gradient_Descent Algorithm</a></span></li></ul></li><li><span><a href=\"#Bootstrap-Algorithm\" data-toc-modified-id=\"Bootstrap-Algorithm-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Bootstrap Algorithm</a></span></li><li><span><a href=\"#Bivariate-Normal-Distribution\" data-toc-modified-id=\"Bivariate-Normal-Distribution-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Bivariate Normal Distribution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation-and-Visualization\" data-toc-modified-id=\"Data-Preparation-and-Visualization-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Data Preparation and Visualization</a></span></li><li><span><a href=\"#Model-Fitting\" data-toc-modified-id=\"Model-Fitting-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Model Fitting</a></span></li></ul></li><li><span><a href=\"#Evolving-Network\" data-toc-modified-id=\"Evolving-Network-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Evolving Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Network-Buliding\" data-toc-modified-id=\"The-Network-Buliding-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>The Network Buliding</a></span></li><li><span><a href=\"#Plot-Network\" data-toc-modified-id=\"Plot-Network-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Plot Network</a></span></li><li><span><a href=\"#The-Confidence-Region\" data-toc-modified-id=\"The-Confidence-Region-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>The Confidence Region</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General syntax to import specific functions in a library: \n",
    "##from (library) import (specific library function)\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "# General syntax to import a library but no functions: \n",
    "### import (library) as (give the library a nickname/alias)\n",
    "import numpy as np # The basic of Python array\n",
    "import matplotlib.pyplot as plt # Plot Packages\n",
    "from scipy import stats # Statistical Models\n",
    "import pandas as pd #this is how I usually import pandas\n",
    "import sys #only needed to determine Python version number\n",
    "import matplotlib #only needed to determine Matplotlib version number\n",
    "from datetime import datetime # datetime we need\n",
    "import random as random # Import the Random Package\n",
    "from imblearn.over_sampling import SMOTE # Import the SMOTE package\n",
    "from collections import Counter # Import Counter package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the data and Data preprocessing\n",
    "raw_df = pd.read_csv(\"DataSet.CSV\") # read the raw data\n",
    "df = raw_df.copy() # copy the raw data\n",
    "df = df[df[\"Detection Date\"] != \"<Null>\"] # remove null\n",
    "s = pd.to_datetime(df[\"Detection Date\"],format= \"%Y/%m/%d\",errors='coerce').notna() # remove odd date\n",
    "df = df[s].copy() # get the right data frame\n",
    "df = df[df[\"Detection Date\"] != \"<Null>\"] # remove null\n",
    "df[\"Detection Date\"] = pd.to_datetime(df[\"Detection Date\"], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "df[\"Submission Date\"] = pd.to_datetime(df[\"Submission Date\"], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "# df = df[df[\"Lab Status\"] != \"Negative ID\"] # remove negative ID\n",
    "# set the date format\n",
    "print(df.shape) # show the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different classes\n",
    "pos_df = df[df[\"Lab Status\"] == \"Positive ID\"] # Positive Class\n",
    "neg_df = df[df[\"Lab Status\"] == \"Negative ID\"] # Negative Class\n",
    "unver_df = df[df[\"Lab Status\"] == \"Unverified\"] # Unverified Class\n",
    "unpro_df = df[df[\"Lab Status\"] == \"Unprocessed\"] # Unprocessed Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the sample which falls between latitude 48.9 - 49.1 and longtitude -122.8 ~ -122.5\n",
    "subdf = df.copy() # copy the data\n",
    "subdf = subdf[subdf[\"Latitude\"] >= 48.9] \n",
    "subdf = subdf[subdf[\"Latitude\"] <= 49.1]\n",
    "subdf = subdf[subdf[\"Longitude\"] >= -122.8]\n",
    "subdf = subdf[subdf[\"Longitude\"] <= -122.5]\n",
    "# set the range of latitude and longtitude\n",
    "print(subdf) # print the data frame\n",
    "\n",
    "# Create different subclasses\n",
    "subpos_df = subdf[subdf[\"Lab Status\"] == \"Positive ID\"] # Positive Class\n",
    "subneg_df = subdf[subdf[\"Lab Status\"] == \"Negative ID\"] # Negative Class\n",
    "subunver_df = subdf[subdf[\"Lab Status\"] == \"Unverified\"] # Unverified Class\n",
    "subunpro_df = subdf[subdf[\"Lab Status\"] == \"Unprocessed\"] # Unprocessed Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sample by their lab status\n",
    "p1 = plt.scatter(subpos_df[\"Longitude\"],subpos_df[\"Latitude\"], marker = \"o\", c = \"b\")\n",
    "p2 = plt.scatter(subneg_df[\"Longitude\"],subneg_df[\"Latitude\"], marker = \"o\", c = \"r\")\n",
    "p3 = plt.scatter(subunver_df[\"Longitude\"],subunver_df[\"Latitude\"], marker = \"o\", c = \"y\")\n",
    "p4 = plt.scatter(subunpro_df[\"Longitude\"],subunpro_df[\"Latitude\"], marker = \"o\", c = \"g\")\n",
    "plt.xlabel(\"The Longitude\")\n",
    "plt.ylabel(\"The Latitude\")\n",
    "plt.title(\"Latitude: 48.9 - 49.1, Longtitude: -122.8 ~ -122.5\")\n",
    "plt.legend([p1, p2, p3, p4], [\"Positive\", \"Negative\", \"Unverified\", \"Unprocessed\"], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year list\n",
    "YearNumberDict = dict()\n",
    "for date in pos_df[\"Detection Date\"]:\n",
    "    if date.year not in YearNumberDict:\n",
    "        YearNumberDict[date.year] = 1\n",
    "    else:\n",
    "        YearNumberDict[date.year] += 1\n",
    "print(YearNumberDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN data set\n",
    "KNNdf = subdf[[\"Latitude\", \"Longitude\"]]\n",
    "KNNList = KNNdf.values.tolist()\n",
    "\n",
    "### Create the complete data set\n",
    "Comdf = subdf.copy()\n",
    "\n",
    "### Drop index in the data set\n",
    "Comdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return np.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply KNN to change all unverified and unprocessed values to labeled values\n",
    "for i in range(Comdf.shape[0]):\n",
    "    if (Comdf.iloc[i][\"Lab Status\"] == \"Unverified\") or (Comdf.iloc[i][\"Lab Status\"] == \"Unprocessed\"):\n",
    "        neighbors = get_neighbors(KNNList, KNNList[i], 30)\n",
    "        for neighbor in neighbors:\n",
    "            cond1 = neighbor != KNNList[i]\n",
    "            cond2 = (Comdf[Comdf[\"Latitude\"] == neighbor[0]][\"Lab Status\"].to_string(index=False).strip() == \"Positive ID\")\n",
    "            cond3 = (Comdf[Comdf[\"Latitude\"] == neighbor[0]][\"Lab Status\"].to_string(index=False).strip() == \"Negative ID\")\n",
    "            if cond1 and (cond2 or cond3):\n",
    "                Comdf.loc[i, \"Lab Status\"] = Comdf[Comdf[\"Latitude\"] == neighbor[0]][\"Lab Status\"].to_string(index=False).strip()  \n",
    "                break                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the Algorithm to the Whole Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Whole data set\n",
    "Wholedf = raw_df.copy()\n",
    "### KNN data set\n",
    "WholeKNNdf = raw_df[[\"Latitude\", \"Longitude\"]]\n",
    "WholeKNNList = WholeKNNdf.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN Process\n",
    "for i in range(Wholedf.shape[0]):\n",
    "    if (Wholedf.iloc[i][\"Lab Status\"] == \"Unverified\") or (Wholedf.iloc[i][\"Lab Status\"] == \"Unprocessed\"):\n",
    "        neighbors = get_neighbors(WholeKNNList, WholeKNNList[i], 30)\n",
    "        for neighbor in neighbors:\n",
    "            cond1 = neighbor != WholeKNNList[i]\n",
    "            cond2 = (Wholedf[Wholedf[\"Latitude\"] == neighbor[0]][\"Lab Status\"].to_string(index=False).strip() == \"Positive ID\")\n",
    "            cond3 = (Wholedf[Wholedf[\"Latitude\"] == neighbor[0]][\"Lab Status\"].to_string(index=False).strip() == \"Negative ID\")\n",
    "            if cond1 and (cond2 or cond3):\n",
    "                Wholedf.loc[i, \"Lab Status\"] = Wholedf[Wholedf[\"Latitude\"] == neighbor[0]][\"Lab Status\"].to_string(index=False).strip()  \n",
    "                break         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the data by label\n",
    "pos_df_com = Comdf[Comdf[\"Lab Status\"] == \"Positive ID\"] # Positive Class\n",
    "neg_df_com = Comdf[Comdf[\"Lab Status\"] == \"Negative ID\"] # Negative Class\n",
    "unver_df_com = Comdf[Comdf[\"Lab Status\"] == \"Unverified\"] # Unverified Class\n",
    "unpro_df_com = Comdf[Comdf[\"Lab Status\"] == \"Unprocessed\"] # Unprocessed Class\n",
    "### Create year list\n",
    "YearNumberDict = dict()\n",
    "for date in pos_df_com[\"Detection Date\"]:\n",
    "    if date.year not in YearNumberDict:\n",
    "        YearNumberDict[date.year] = 1\n",
    "    else:\n",
    "        YearNumberDict[date.year] += 1\n",
    "print(YearNumberDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization After KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the sample by their lab status\n",
    "mp1 = plt.scatter(pos_df_com[\"Longitude\"],pos_df_com[\"Latitude\"], marker = \"o\", c = \"b\")\n",
    "mp2 = plt.scatter(neg_df_com[\"Longitude\"],neg_df_com[\"Latitude\"], marker = \"o\", c = \"r\")\n",
    "mp3 = plt.scatter(unver_df_com[\"Longitude\"],unver_df_com[\"Latitude\"], marker = \"o\", c = \"y\")\n",
    "mp4 = plt.scatter(unpro_df_com[\"Longitude\"],unpro_df_com[\"Latitude\"], marker = \"o\", c = \"g\")\n",
    "plt.xlabel(\"The Longitude\")\n",
    "plt.ylabel(\"The Latitude\")\n",
    "plt.title(\"Latitude: 48.9 - 49.1, Longtitude: -122.8 ~ -122.5\")\n",
    "plt.legend([mp1, mp2], [\"Positive\", \"Negative\"], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-Means algorithm fitting process\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "kmeans.fit(subdf[[\"Latitude\", \"Longitude\"]])\n",
    "y_kmeans = kmeans.predict(subdf[[\"Latitude\", \"Longitude\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data visualization after k-means\n",
    "p = plt.scatter(subdf[\"Longitude\"], subdf[\"Latitude\"], c = y_kmeans, s = 50, cmap='viridis')\n",
    "plt.xlabel(\"The Longitude\")\n",
    "plt.ylabel(\"The Latitude\")\n",
    "plt.title(\"Latitude: 48.9 - 49.1, Longtitude: -122.8 ~ -122.5\")\n",
    "plt.legend(y_kmeans, [\"Cluster 1\", \"Cluster 2\"], loc = \"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Minority Oversampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Depart the data set\n",
    "Location = subdf[[\"Latitude\", \"Longitude\"]]\n",
    "Status = subdf[\"Lab Status\"]\n",
    "\n",
    "Status[Status != \"Positive ID\"] = \"NonPositive ID\"\n",
    "\n",
    "### summarize class distribution\n",
    "counter = Counter(subdf[\"Lab Status\"])\n",
    "print(counter)\n",
    "\n",
    "### transform the dataset\n",
    "oversample = SMOTE(random_state = 42)\n",
    "Location, Status = oversample.fit_resample(Location, Status)\n",
    "\n",
    "### Creating a new Oversampling Data Frame\n",
    "Location[\"Lab Status\"] = Status\n",
    "subdf_oversampler = Location.copy()\n",
    "\n",
    "### summarize class distribution\n",
    "counter = Counter(subdf_oversampler[\"Lab Status\"])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label the data\n",
    "pos_oversampler = subdf_oversampler[subdf_oversampler[\"Lab Status\"] == \"Positive ID\"]\n",
    "non_oversampler = subdf_oversampler[subdf_oversampler[\"Lab Status\"] == \"NonPositive ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data visualization after SMOTE\n",
    "plt.scatter(pos_oversampler[\"Latitude\"], pos_oversampler[\"Longitude\"], cmap='viridis')\n",
    "plt.scatter(non_oversampler[\"Latitude\"], non_oversampler[\"Longitude\"], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Processes and Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season and Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Season Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create season date\n",
    "seasondf = Comdf.copy()\n",
    "seasons = [\"4_Winter\", \"4_Winter\", \"1_Spring\", \"1_Spring\", \"1_Spring\",\n",
    "           \"2_Summer\", \"2_Summer\", \"2_Summer\", \"3_Fall\", \"3_Fall\", \"3_Fall\", \"4_Winter\"]\n",
    "month_to_season = dict(zip(range(1,13), seasons))\n",
    "season = list()\n",
    "for date in seasondf[\"Detection Date\"]:\n",
    "    season.append(str(date.year) + \"-\" + month_to_season[date.month])\n",
    "seasondf[\"season\"] = season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create season dictionary\n",
    "seasonposdf = seasondf[Comdf[\"Lab Status\"] == \"Positive ID\"] # Positive Class\n",
    "seasoncounter = Counter(seasonposdf[\"season\"])\n",
    "SeasonNumberDict = dict(sorted(seasoncounter.items()))\n",
    "print(SeasonNumberDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the dict of year and number\n",
    "YearNumberDict = {2019: 4, 2020: 15}\n",
    "print(YearNumberDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the bar of Year and Number\n",
    "plt.bar(YearNumberDict.keys(), YearNumberDict.values(), align = 'center',color='steelblue', alpha = 0.8)\n",
    "plt.xlabel(\"The Year of Detection\")\n",
    "plt.ylabel(\"The Number of Detection\")\n",
    "plt.title(\"The Year and Number of Detection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the bar of Season and Number\n",
    "Seasonshort = ['2019-Sum', '2019_Fall', '2019_Win', '2020_Spr', '2020_Sum', '2020_Fall']\n",
    "plt.bar(Seasonshort, SeasonNumberDict.values(), align = 'center', color='steelblue')\n",
    "plt.xlabel(\"The Season of Detection\")\n",
    "plt.ylabel(\"The Number of Detection\")\n",
    "plt.title(\"The Season and Number of Detection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Postive season data\n",
    "posseasondf = seasondf[seasondf[\"Lab Status\"] == \"Positive ID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Random Variables from normal distribution\n",
    "from scipy.stats import multivariate_normal\n",
    "meanlist1 = [posseasondf[\"Longitude\"][96],posseasondf[\"Latitude\"][96]]\n",
    "meanlist2 = [posseasondf[\"Longitude\"][101],posseasondf[\"Latitude\"][101]]\n",
    "meanlist3 = [posseasondf[\"Longitude\"][103],posseasondf[\"Latitude\"][103]]\n",
    "meanlist4 = [posseasondf[\"Longitude\"][106],posseasondf[\"Latitude\"][106]]\n",
    "meanlist5 = [posseasondf[\"Longitude\"][107],posseasondf[\"Latitude\"][107]]\n",
    "meanlist6 = [posseasondf[\"Longitude\"][108],posseasondf[\"Latitude\"][108]]\n",
    "meanlist7 = [posseasondf[\"Longitude\"][111],posseasondf[\"Latitude\"][111]]\n",
    "meanlist8 = [posseasondf[\"Longitude\"][115],posseasondf[\"Latitude\"][115]]\n",
    "meanlist9 = [posseasondf[\"Longitude\"][121],posseasondf[\"Latitude\"][121]]\n",
    "\n",
    "### Generate random variables\n",
    "sample1 = multivariate_normal.rvs(meanlist1, cov, 3)\n",
    "Winter2020 = sample1\n",
    "print(Winter2020)\n",
    "sample2 = multivariate_normal.rvs(meanlist2, cov, 2)\n",
    "Winter2020 = np.append(Winter2020, sample2)\n",
    "sample3 = multivariate_normal.rvs(meanlist3, cov, 2)\n",
    "Winter2020 = np.append(Winter2020, sample3)\n",
    "sample4 = multivariate_normal.rvs(meanlist4, cov, 2)\n",
    "Winter2020 = np.append(Winter2020, sample4)\n",
    "sample5 = multivariate_normal.rvs(meanlist5, cov, 2)\n",
    "Winter2020 = np.append(Winter2020, sample5)\n",
    "sample6 = multivariate_normal.rvs(meanlist6, cov, 2)\n",
    "Winter2020 = np.append(Winter2020, sample6)\n",
    "sample7 = multivariate_normal.rvs(meanlist7, cov, 2)\n",
    "Winter2020 = np.append(Winter2020, sample7)\n",
    "sample8 = multivariate_normal.rvs(meanlist8, cov, 2)\n",
    "Winter2020 = np.append(Winter2020, sample8)\n",
    "sample9 = multivariate_normal.rvs(meanlist9, cov, 2)\n",
    "Winter2020 = np.append(Winter2020, sample9)\n",
    "Winter2020 = Winter2020.reshape(19,2)\n",
    "Winter2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Depart the data set\n",
    "Summer2019 = posseasondf[posseasondf[\"season\"] == \"2019-2_Summer\"]\n",
    "Fall2019 = posseasondf[posseasondf[\"season\"] ==\"2019-3_Fall\"]\n",
    "Winter2019 = posseasondf[posseasondf[\"season\"] == \"2019-4_Winter\"]\n",
    "Spring2020 = posseasondf[posseasondf[\"season\"] == \"2020-1_Spring\"]\n",
    "Summer2020 = posseasondf[posseasondf[\"season\"] == \"2029-2_Summer\"]\n",
    "Fall2020 = posseasondf[posseasondf[\"season\"] == \"2020-3_Fall\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Season Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the sample by their lab status from Summer2019 to Fall2020\n",
    "sp1 = plt.scatter(Summer2019[\"Longitude\"],Summer2019[\"Latitude\"], cmap='viridis')\n",
    "sp2 = plt.scatter(Fall2019[\"Longitude\"],Fall2019[\"Latitude\"], cmap='viridis')\n",
    "sp3 = plt.scatter(Winter2019[\"Longitude\"],Winter2019[\"Latitude\"], cmap='viridis')\n",
    "sp4 = plt.scatter(Spring2020[\"Longitude\"],Spring2020[\"Latitude\"], cmap='viridis')\n",
    "sp5 = plt.scatter(Summer2020[\"Longitude\"],Summer2020[\"Latitude\"], cmap='viridis')\n",
    "sp6 = plt.scatter(Fall2020[\"Longitude\"],Fall2020[\"Latitude\"], cmap='viridis')\n",
    "plt.xlabel(\"The Longitude\")\n",
    "plt.ylabel(\"The Latitude\")\n",
    "plt.title(\"Latitude: 48.9 - 49.1, Longtitude: -122.8 ~ -122.5\")\n",
    "plt.legend([sp1, sp2, sp3, sp4, sp5, sp6], [\"Summer2019\", \"Fall2019\",\"Winter2019\",\"Spring2020\",\"Summer2020\",\"Fall2020\"], loc='upper left')     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the sample by their lab status from Summer2019 to Winter2020\n",
    "sp1 = plt.scatter(Summer2019[\"Longitude\"],Summer2019[\"Latitude\"], cmap='viridis')\n",
    "sp2 = plt.scatter(Fall2019[\"Longitude\"],Fall2019[\"Latitude\"], cmap='viridis')\n",
    "sp3 = plt.scatter(Winter2019[\"Longitude\"],Winter2019[\"Latitude\"], cmap='viridis')\n",
    "sp4 = plt.scatter(Spring2020[\"Longitude\"],Spring2020[\"Latitude\"], cmap='viridis')\n",
    "sp5 = plt.scatter(Summer2020[\"Longitude\"],Summer2020[\"Latitude\"], cmap='viridis')\n",
    "sp6 = plt.scatter(Fall2020[\"Longitude\"],Fall2020[\"Latitude\"], cmap='viridis')\n",
    "sp7 = plt.scatter(Winter2020[:,0], Winter2020[:,1], cmap='viridis') \n",
    "plt.xlabel(\"The Longitude\")\n",
    "plt.ylabel(\"The Latitude\")\n",
    "plt.title(\"Latitude: 48.9 - 49.1, Longtitude: -122.8 ~ -122.5\")\n",
    "plt.legend([sp1, sp2, sp3, sp4, sp5, sp6, sp7],\n",
    "           [\"Summer2019\", \"Fall2019\",\"Winter2019\",\"Spring2020\",\"Summer2020\",\"Fall2020\", \"Winter2020\"],\n",
    "           loc='upper left', fontsize = \"xx-small\")       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birth-Death Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the training set\n",
    "m = np.array([1, 3, 4, 9, 10, 19]) # 1,2,1,5,1,9\n",
    "t = np.array([0, 1, 2, 3, 4, 5])\n",
    "\n",
    "### random init\n",
    "random.seed(a = 5)\n",
    "laminit = random.random()\n",
    "print(laminit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the training data \n",
    "from scipy.stats import chisquare\n",
    "chisquare([0,3,1,0,0,1,0,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient_Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gradient descent algorithm iteration function\n",
    "def gradient_descent(m, t, lam, alpha, num_iterations = 10000):\n",
    "    e = 2.71828182846\n",
    "    for i in range(0, num_iterations):\n",
    "        gradient = 0\n",
    "        for j in range(0, len(m) - 1):            \n",
    "            gradient += (1 / len(m)) * (e ** (lam * t[j]) - m[j]) * t[j] * e ** (lam * t[j])\n",
    "        # print(gradient)\n",
    "        lam = lam - alpha * gradient\n",
    "        return(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamhat = gradient_descent(m, t, laminit, alpha = 0.005)\n",
    "print(lamhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bootstrap Process\n",
    "from scipy.stats import poisson\n",
    "np.random.seed(seed=233423)\n",
    "e = 2.71828182846 # define e\n",
    "G = 10000 # bootstrap replications\n",
    "lamstar = np.zeros(G) # bootstrap replications lambda\n",
    "for i in range(0, G):\n",
    "    mstar = np.array([0,0,0,0,0,0]) # init mstar\n",
    "    for j in range(0, len(m) - 1):\n",
    "        if j == 0:\n",
    "            mstar[j] = 1\n",
    "        else:\n",
    "            mstar[j] = mstar[j - 1] + poisson.rvs((j * (j + 1) / 2) * lamhat)\n",
    "    lamstar[i] = gradient_descent(mstar, t, laminit, alpha = 0.005)\n",
    "lamstar.sort()\n",
    "# print(lamstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get quantiles\n",
    "print(lamstar[249])\n",
    "print(lamstar[4999])\n",
    "print(lamstar[9749])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get positive data\n",
    "pos_oversampler = subdf_oversampler[subdf_oversampler[\"Lab Status\"] == \"Positive ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the data\n",
    "plt.scatter(pos_oversampler[\"Latitude\"], pos_oversampler[\"Longitude\"], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Consider the sample which falls between latitude 48.96 - 49.0 and longtitude -122.725 ~ -122.675\n",
    "smalldf = pos_oversampler.copy() # copy the data\n",
    "smalldf = smalldf[smalldf[\"Latitude\"] >= 48.96] \n",
    "smalldf = smalldf[smalldf[\"Latitude\"] <= 49.01]\n",
    "smalldf = smalldf[smalldf[\"Longitude\"] >= -122.583]\n",
    "smalldf = smalldf[smalldf[\"Longitude\"] <= -122.57] \n",
    "# set the range of latitude and longtitude\n",
    "\n",
    "### Plot the data\n",
    "plt.scatter(smalldf[\"Latitude\"], smalldf[\"Longitude\"], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The MLE of normal distribution \n",
    "posX = [pos_df_com[\"Latitude\"], pos_df_com[\"Longitude\"]]\n",
    "mean = np.mean(posX, axis = 1)\n",
    "cov = np.cov(posX, rowvar = 1)\n",
    "print(mean)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shapiro test\n",
    "normaldf = pd.read_csv(\"Pos.CSV\") # read the raw data\n",
    "posnormal = [normaldf[\"Latitude\"], normaldf[\"Longitude\"]]\n",
    "shapiro_test = stats.shapiro(posX)\n",
    "shapiro_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolving Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Network Buliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normal Data generation\n",
    "np.random.seed(seed = 114514)\n",
    "mnew = poisson.rvs(28 * lamhat)\n",
    "print(mnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time data and real data generation\n",
    "t_overall = np.array(range(0,100))\n",
    "m_overall = np.exp(lamhat * t_overall)\n",
    "plt.plot(t_overall, m_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add nodes\n",
    "import networkx as nx # 1,2,1,5,1,9\n",
    "G = nx.Graph() # Create network\n",
    "G.add_node(0, season = \"2019-summer\")\n",
    "G.add_node(1, season = \"2019-Fall\")\n",
    "G.add_node(2, season = \"2019-Fall\")\n",
    "G.add_node(3, season = \"2019-Winter\")\n",
    "for i in range(4,9):\n",
    "    G.add_node(i, season = \"2020-Spring\")\n",
    "G.add_node(9, season = \"2019-Summer\")\n",
    "for i in range(10,19):\n",
    "    G.add_node(i, season = \"2020-Fall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonName = [\"2019-summer\", \"2019-Fall\", \"2019-Winter\", \"2020-Spring\", \"2019-Summer\", \"2020-Fall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add edges\n",
    "for node1 in G.nodes:\n",
    "    for node2 in G.nodes:\n",
    "        if seasonName.index(G.nodes[node1][\"season\"]) + 1 == seasonName.index(G.nodes[node2][\"season\"]):\n",
    "            G.add_edge(node1,node2)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_shell(G, with_labels = False, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sample by their lab status\n",
    "mp1 = plt.scatter(pos_df_com[\"Longitude\"],pos_df_com[\"Latitude\"], marker = \"o\", c = \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "meanlist = [pos_df_com[\"Longitude\"][0],pos_df_com[\"Latitude\"][0]]\n",
    "sample1 = multivariate_normal.rvs(meanlist, cov, 19)\n",
    "Winter2020 = sample1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Confidence Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### confidence_ellipse function\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "    if x.size != y.size:\n",
    "        raise ValueError(\"x and y must be the same size\")\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensionl dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the stdandard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    print(scale_x)\n",
    "    mean_x = np.mean(x)\n",
    "    \n",
    "\n",
    "    # calculating the stdandard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    print(scale_y)\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply the confidence_ellipse funtion to get the ellipse\n",
    "fig, ax_nstd = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax_nstd.scatter(pos_df_com[\"Longitude\"],pos_df_com[\"Latitude\"], s=0.5)\n",
    "\n",
    "confidence_ellipse(pos_df_com[\"Longitude\"], pos_df_com[\"Latitude\"], ax_nstd, n_std=1,\n",
    "                   label=r'$1\\sigma$', edgecolor='firebrick')\n",
    "confidence_ellipse(pos_df_com[\"Longitude\"], pos_df_com[\"Latitude\"], ax_nstd, n_std=2,\n",
    "                   label=r'$2\\sigma$', edgecolor='fuchsia', linestyle='--')\n",
    "confidence_ellipse(pos_df_com[\"Longitude\"], pos_df_com[\"Latitude\"], ax_nstd, n_std=3,\n",
    "                   label=r'$3\\sigma$', edgecolor='blue', linestyle=':')\n",
    "\n",
    "ax_nstd.scatter(mean[1], mean[0], c='red', s=3)\n",
    "ax_nstd.set_title('Different standard deviations')\n",
    "ax_nstd.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transfer the degree to the kilometers\n",
    "print((0.0649986735194373 + 0.02543062752178979)/ 2 * 111 )\n",
    "print((0.1299973470388746 + 0.05086125504357958)/ 2 * 111 )\n",
    "print((0.19499602055831192 + 0.07629188256536937)/ 2 * 111 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
